{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "048d6f83-c2c6-4f8e-94ab-6f563d574485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pymc as pm\n",
    "from pymc import do, observe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder\n",
    "from pytensor import tensor as pt\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import itertools as it\n",
    "import country_converter as cc\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e02003-11ad-4627-afcb-ccc5082eb5f7",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fe84ea1-88f9-48ad-8cc3-e008dde0b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/processed/burke_ortizbobea_integrated_dataset_with_custom_temp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf41f3ab-1701-4c8b-bbe1-5651ffd4c488",
   "metadata": {},
   "source": [
    "# Remove all data for countries where one of the variables is entirely missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75ad7dad-b5ff-4992-9a34-d73fbc967a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "{'MLT', 'NOR', 'SYC', 'SGP', 'BHR', 'COD', 'JAM', 'ISR', 'ROU'}\n",
      "Removed 504 rows for completely missing country data.\n"
     ]
    }
   ],
   "source": [
    "country_temp_data = {}\n",
    "country_precip_data = {}\n",
    "country_gdp_data = {}\n",
    "country_tfp_data = {}\n",
    "\n",
    "for row in data.iterrows():\n",
    "    country = row[1].country\n",
    "    if country not in country_temp_data:\n",
    "        country_temp_data[country] = []\n",
    "    if country not in country_precip_data:\n",
    "        country_precip_data[country] = []\n",
    "    if country not in country_gdp_data:\n",
    "        country_gdp_data[country] = []\n",
    "    if country not in country_tfp_data:\n",
    "        country_tfp_data[country] = []\n",
    "\n",
    "    country_temp_data[country].append(row[1].unweighted_temp)\n",
    "    country_precip_data[country].append(row[1].unweighted_precip)\n",
    "    country_gdp_data[country].append(row[1].ln_gdp_change)\n",
    "    country_tfp_data[country].append(row[1].ln_tfp_change)\n",
    "\n",
    "countries_missing_temp = [country for country in country_temp_data if all(np.isnan(country_temp_data[country]))]\n",
    "countries_missing_precip = [country for country in country_precip_data if all(np.isnan(country_precip_data[country]))]\n",
    "countries_missing_gdp = [country for country in country_gdp_data if all(np.isnan(country_gdp_data[country]))]\n",
    "countries_missing_tfp = [country for country in country_tfp_data if all(np.isnan(country_tfp_data[country]))]\n",
    "\n",
    "countries_to_remove = set(countries_missing_temp + countries_missing_precip + countries_missing_gdp + countries_missing_tfp)\n",
    "print(countries_to_remove)\n",
    "\n",
    "indices_to_drop = []\n",
    "for index, row in enumerate(data.itertuples()):\n",
    "    if row.country in countries_to_remove:\n",
    "        indices_to_drop.append(index)\n",
    "        \n",
    "data_len_before = len(data)\n",
    "data = data.drop(indices_to_drop)\n",
    "data = data.reset_index()\n",
    "print(f\"Removed {data_len_before - len(data)} rows for completely missing country data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b6e78-62ac-465a-92ce-6fb1d3e4c729",
   "metadata": {},
   "source": [
    "# Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12b0b2a2-2337-4146-b9c0-317cfd9058b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_scaler, gdp_scaler, temp_scaler, tfp_scaler = StandardScaler(), StandardScaler(), StandardScaler(), StandardScaler()\n",
    "precip_scaled = precip_scaler.fit_transform(np.array(data.unweighted_precip).reshape(-1,1)).flatten()\n",
    "gdp_scaled = gdp_scaler.fit_transform(np.array(data.ln_gdp_change).reshape(-1,1)).flatten()\n",
    "temp_scaled = temp_scaler.fit_transform(np.array(data.unweighted_temp).reshape(-1,1)).flatten()\n",
    "tfp_scaled = tfp_scaler.fit_transform(np.array(data.ln_tfp_change).reshape(-1,1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd14212-fd50-45c9-b10e-34f529f5be3d",
   "metadata": {},
   "source": [
    "# Year and country fixed effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fab6cf06-f512-4804-83b3-adc3f5f53a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = len(data.year)\n",
    "year_mult_mat = [np.zeros(data_len) for year in set(data.year)]\n",
    "country_mult_mat = [np.zeros(data_len) for country in set(data.country)]\n",
    "country_index = -1\n",
    "curr_country = \"\"\n",
    "for row_index, row in enumerate(data.itertuples()):\n",
    "    if row.country != curr_country:\n",
    "        country_index += 1\n",
    "        curr_country = row.country\n",
    "    year_index = row.year - 1960\n",
    "    country_mult_mat[country_index][row_index] = 1\n",
    "    year_mult_mat[year_index][row_index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6bca9-1cc4-477a-b934-1e06524c260f",
   "metadata": {},
   "source": [
    "# Build integrated Burke-OrtizBobea model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "950788b8-899c-45a1-a9e2-c867a2f9cf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hayden_freedman/pymc_dev.venv/lib/python3.10/site-packages/pymc/model/core.py:1323: ImputationWarning: Data in precip_posterior contains missing values and will be automatically imputed from the sampling distribution.\n",
      "  warnings.warn(impute_message, ImputationWarning)\n",
      "/home/hayden_freedman/pymc_dev.venv/lib/python3.10/site-packages/pymc/model/core.py:1323: ImputationWarning: Data in gdp_posterior contains missing values and will be automatically imputed from the sampling distribution.\n",
      "  warnings.warn(impute_message, ImputationWarning)\n",
      "/home/hayden_freedman/pymc_dev.venv/lib/python3.10/site-packages/pymc/model/core.py:1323: ImputationWarning: Data in tfp_posterior contains missing values and will be automatically imputed from the sampling distribution.\n",
      "  warnings.warn(impute_message, ImputationWarning)\n",
      "Sampling: [country_coefs_precip_prior, country_coefs_temp_prior, gdp_country_coefs, gdp_posterior_observed, gdp_posterior_unobserved, gdp_std, gdp_year_coefs, precip_gdp_coef, precip_gdp_coef2, precip_gdp_intercept, precip_posterior_observed, precip_posterior_unobserved, precip_std, precip_tfp_coef, precip_tfp_coef2, precip_tfp_intercept, temp_gdp_coef, temp_gdp_coef2, temp_gdp_intercept, temp_posterior, temp_std, temp_tfp_coef, temp_tfp_coef2, temp_tfp_intercept, tfp_country_coefs, tfp_posterior_observed, tfp_posterior_unobserved, tfp_std, tfp_year_coefs]\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [country_coefs_temp_prior, temp_std, temp_gdp_coef, temp_gdp_coef2, temp_gdp_intercept, temp_tfp_coef, temp_tfp_coef2, temp_tfp_intercept, country_coefs_precip_prior, precip_std, precip_posterior_unobserved, precip_gdp_coef, precip_gdp_coef2, precip_gdp_intercept, precip_tfp_coef, precip_tfp_coef2, precip_tfp_intercept, gdp_year_coefs, gdp_country_coefs, gdp_std, gdp_posterior_unobserved, tfp_year_coefs, tfp_country_coefs, tfp_std, tfp_posterior_unobserved]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='41' class='' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.51% [41/8000 00:39&lt;2:07:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Not enough samples to build a trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m     tfp_posterior \u001b[38;5;241m=\u001b[39m pm\u001b[38;5;241m.\u001b[39mNormal(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfp_posterior\u001b[39m\u001b[38;5;124m'\u001b[39m, mu\u001b[38;5;241m=\u001b[39mtfp_prior, sigma\u001b[38;5;241m=\u001b[39mtfp_std, observed\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mln_tfp_change\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     68\u001b[0m     prior \u001b[38;5;241m=\u001b[39m pm\u001b[38;5;241m.\u001b[39msample_prior_predictive()\n\u001b[0;32m---> 69\u001b[0m     trace \u001b[38;5;241m=\u001b[39m \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     posterior \u001b[38;5;241m=\u001b[39m pm\u001b[38;5;241m.\u001b[39msample_posterior_predictive(trace, extend_inferencedata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/temp_precip_gdp_tfp_integrated.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m buff:\n",
      "File \u001b[0;32m~/pymc_dev.venv/lib/python3.10/site-packages/pymc/sampling/mcmc.py:827\u001b[0m, in \u001b[0;36msample\u001b[0;34m(draws, tune, chains, cores, random_seed, progressbar, step, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, model, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m t_sampling \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t_start\n\u001b[1;32m    825\u001b[0m \u001b[38;5;66;03m# Packaging, validating and returning the result was extracted\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;66;03m# into a function to make it easier to test and refactor.\u001b[39;00m\n\u001b[0;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_sampling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43midata_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midata_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pymc_dev.venv/lib/python3.10/site-packages/pymc/sampling/mcmc.py:858\u001b[0m, in \u001b[0;36m_sample_return\u001b[0;34m(run, traces, tune, t_sampling, discard_tuned_samples, compute_convergence_checks, return_inferencedata, keep_warning_stat, idata_kwargs, model)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# Pick and slice chains to keep the maximum number of samples\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m discard_tuned_samples:\n\u001b[0;32m--> 858\u001b[0m     traces, length \u001b[38;5;241m=\u001b[39m \u001b[43m_choose_chains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    860\u001b[0m     traces, length \u001b[38;5;241m=\u001b[39m _choose_chains(traces, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/pymc_dev.venv/lib/python3.10/site-packages/pymc/backends/base.py:601\u001b[0m, in \u001b[0;36m_choose_chains\u001b[0;34m(traces, tune)\u001b[0m\n\u001b[1;32m    599\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trace) \u001b[38;5;241m-\u001b[39m tune) \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m traces]\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(lengths):\n\u001b[0;32m--> 601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough samples to build a trace.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    603\u001b[0m idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(lengths)\n\u001b[1;32m    604\u001b[0m l_sort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(lengths)[idxs]\n",
      "\u001b[0;31mValueError\u001b[0m: Not enough samples to build a trace."
     ]
    }
   ],
   "source": [
    "with pm.Model() as model:\n",
    "\n",
    "    country_coefs_temp_prior = pt.expand_dims(pm.Normal(\"country_coefs_temp_prior\", 0, 1, shape=(len(set(data.country)))),axis=1)\n",
    "    temp_prior = pm.Deterministic(\"temp_prior\",pt.sum(country_coefs_temp_prior*country_mult_mat,axis=0))    \n",
    "    temp_std = pm.HalfNormal(\"temp_std\", 1)\n",
    "    temp_posterior = pm.Normal(\"temp_posterior\", temp_prior, temp_std, observed=temp_scaled)\n",
    "    \n",
    "    temp_gdp_coef = pm.Normal('temp_gdp_coef',0,1)\n",
    "    temp_gdp_coef2 = pm.Normal('temp_gdp_coef2',0,1)\n",
    "\n",
    "    temp_tfp_coef = pm.Normal('temp_tfp_coef',0,1)\n",
    "    temp_tfp_coef2 = pm.Normal('temp_tfp_coef2',0,1)\n",
    "    temp_tfp_intercept = pm.Normal('temp_tfp_intercept',0,1)\n",
    "\n",
    "    country_coefs_precip_prior = pt.expand_dims(pm.Normal(\"country_coefs_precip_prior\", 0, 1, shape=(len(set(data.country)))),axis=1)\n",
    "    precip_prior = pm.Deterministic(\"precip_prior\",pt.sum(country_coefs_precip_prior*country_mult_mat,axis=0))\n",
    "    precip_std = pm.HalfNormal(\"precip_std\", 1)\n",
    "    precip_posterior = pm.Normal(\"precip_posterior\", precip_prior, precip_std, observed=precip_scaled)\n",
    "\n",
    "    precip_gdp_coef = pm.Normal('precip_gdp_coef',0,1)\n",
    "    precip_gdp_coef2 = pm.Normal('precip_gdp_coef2',0,1)\n",
    "\n",
    "    precip_tfp_coef = pm.Normal('precip_tfp_coef',0,1)\n",
    "    precip_tfp_coef2 = pm.Normal('precip_tfp_coef2',0,1)\n",
    "\n",
    "    gdp_year_coefs = pt.expand_dims(pm.Normal(\"gdp_year_coefs\", 0, 10, shape=(len(set(data.year)))),axis=1)\n",
    "    gdp_year_fixed_effects = pm.Deterministic(\"gdp_year_fixed_effects\",pt.sum(gdp_year_coefs*year_mult_mat,axis=0))\n",
    "    gdp_country_coefs = pt.expand_dims(pm.Normal(\"gdp_country_coefs\", 0, 10, shape=(len(set(data.country)))),axis=1)\n",
    "    gdp_country_fixed_effects = pm.Deterministic(\"gdp_country_fixed_effects\",pt.sum(gdp_country_coefs*country_mult_mat,axis=0))\n",
    "\n",
    "    gdp_intercept = pm.Normal(\"gdp_intercept\", 0, 1)\n",
    "    \n",
    "    gdp_prior = pm.Deterministic(\n",
    "        \"gdp_prior\", \n",
    "        gdp_intercept + \n",
    "        (temp_gdp_coef * temp_posterior) + \n",
    "        (temp_gdp_coef2 * pt.sqr(temp_posterior)) +\n",
    "        (precip_gdp_coef * precip_posterior) +\n",
    "        (precip_gdp_coef2 * pt.sqr(precip_posterior)) +\n",
    "        gdp_year_fixed_effects +\n",
    "        gdp_country_fixed_effects\n",
    "    )\n",
    "    gdp_std = pm.HalfNormal('gdp_std', sigma=10)\n",
    "    gdp_posterior = pm.Normal('gdp_posterior', mu=gdp_prior, sigma=gdp_std, observed=data[\"ln_gdp_change\"])\n",
    "\n",
    "    # tfp_year_coefs = pt.expand_dims(pm.Normal(\"tfp_year_coefs\", 0, 10, shape=(len(set(data.year)))),axis=1)\n",
    "    # tfp_year_fixed_effects = pm.Deterministic(\"tfp_year_fixed_effects\",pt.sum(tfp_year_coefs*year_mult_mat,axis=0))\n",
    "    # tfp_country_coefs = pt.expand_dims(pm.Normal(\"tfp_country_coefs\", 0, 10, shape=(len(set(data.country)))),axis=1)\n",
    "    # tfp_country_fixed_effects = pm.Deterministic(\"tfp_country_fixed_effects\",pt.sum(tfp_country_coefs*country_mult_mat,axis=0))\n",
    "    \n",
    "    # tfp_prior = pm.Deterministic(\n",
    "    #     \"tfp_prior\", \n",
    "    #     tfp_intercept + \n",
    "    #     (temp_tfp_coef * temp_posterior) + \n",
    "    #     (temp_tfp_coef2 * pt.sqr(temp_posterior)) +\n",
    "    #     precip_tfp_intercept +\n",
    "    #     (precip_tfp_coef * precip_posterior) +\n",
    "    #     (precip_tfp_coef2 * pt.sqr(precip_posterior)) +\n",
    "    #     tfp_year_fixed_effects +\n",
    "    #     tfp_country_fixed_effects\n",
    "    # )\n",
    "    \n",
    "    # tfp_std = pm.HalfNormal('tfp_std', sigma=10)\n",
    "    # tfp_posterior = pm.Normal('tfp_posterior', mu=tfp_prior, sigma=tfp_std, observed=data[\"ln_tfp_change\"])\n",
    "    \n",
    "    prior = pm.sample_prior_predictive()\n",
    "    trace = pm.sample()\n",
    "    posterior = pm.sample_posterior_predictive(trace, extend_inferencedata=True)\n",
    "\n",
    "with open ('../models/burke_unweighted_temp.pkl', 'wb') as buff:\n",
    "    pkl.dump ({\n",
    "        \"prior\": prior, \n",
    "        \"trace\": trace, \n",
    "        \"posterior\": posterior,\n",
    "        \"temp_scaler\": temp_scaler,\n",
    "        \"precip_scaler\": precip_scaler,\n",
    "        \"gdp_scaler\": gdp_scaler\n",
    "    }, buff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dadd88b-1596-4814-a770-d2fddb542459",
   "metadata": {},
   "source": [
    "# Create integrated dataset with unweighted temp for burke and ortiz-bobea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04cc4c17-529f-4ac7-b7d6-57689955741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item(data):\n",
    "    try:\n",
    "        return data.item()\n",
    "    except AttributeError:\n",
    "        return np.NaN\n",
    "    except ValueError:\n",
    "        return np.NaN\n",
    "\n",
    "integrated_data = []\n",
    "\n",
    "unweighted_temp_file = pd.read_csv(\"../data/burke/data/input/custom_monthly_unweighted_temp_by_country.csv\")\n",
    "unweighted_precip_file = pd.read_csv(\"../data/burke/data/input/custom_monthly_unweighted_precip_by_country.csv\")\n",
    "tfp_file = pd.read_csv(\"../data/ortiz-bobea/data2/regdata_preferred_case.csv\")\n",
    "gdp_file = pd.read_csv(\"../data/burke/data/input/GrowthClimateDataset.csv\")\n",
    "\n",
    "all_countries = sorted(set(tfp_data[\"ISO3\"]).intersection(set(gdp_data[\"iso\"])))\n",
    "all_years = set(tfp_data[\"year\"]).union(set(gdp_data[\"year\"]))\n",
    "\n",
    "for country in all_countries:\n",
    "    for year in all_years:\n",
    "        unweighted_temp = get_item(unweighted_temp_file.loc[(unweighted_temp_file[\"Country\"] == country) & (unweighted_temp_file[\"Year\"] == year)][\"Mean_Temp\"])\n",
    "        unweighted_precip = get_item(unweighted_precip_file.loc[(unweighted_precip_file[\"Country\"] == country) & (unweighted_precip_file[\"Year\"] == year)][\"Unweighted_Precipitation\"])\n",
    "        gdp = get_item(gdp_file.loc[(gdp_data[\"iso\"] == country) & (gdp_file[\"year\"] == year)][\"growthWDI\"])\n",
    "        tfp = get_item(tfp_file.loc[(tfp_data[\"ISO3\"] == country) & (tfp_file[\"year\"] == year)][\"fd_log_tfp\"])\n",
    "        integrated_data.append([country, year, unweighted_temp, unweighted_precip, gdp, tfp])\n",
    "\n",
    "with open(\"../data/processed/burke_ortizbobea_integrated_dataset_with_custom_temp.csv\", \"w\") as write_file:\n",
    "    writer = csv.writer(write_file)\n",
    "    writer.writerow([\"country\",\"year\",\"unweighted_temp\",\"unweighted_precip\",\"ln_gdp_change\",\"ln_tfp_change\"])\n",
    "    for row in integrated_data:\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
